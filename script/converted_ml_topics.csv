chapter_name,source,text
What is Linear Regression Algorithm?,ML_notes_csv,"Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such assales, salary, age, product price,etc. Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable. The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image: Mathematically, we can represent a linear regression as: Here, Y= Dependent Variable (Target Variable)X= Independent Variable (predictor Variable)a0= intercept of the line (Gives an additional degree of freedom)a1 = Linear regression coefficient (scale factor to each input value). = random error The values for x and y variables are training datasets for Linear Regression model representation."
How do you interpret a linear regression model?,ML_notes_csv,"Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such assales, salary, age, product price,etc. Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable. The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image: Mathematically, we can represent a linear regression as: Here, Y= Dependent Variable (Target Variable)X= Independent Variable (predictor Variable)a0= intercept of the line (Gives an additional degree of freedom)a1 = Linear regression coefficient (scale factor to each input value). = random error The values for x and y variables are training datasets for Linear Regression model representation."
What are the basic assumptions of the Linear Regression Algorithm?,ML_notes_csv,"Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such assales, salary, age, product price,etc. Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable. The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image: Mathematically, we can represent a linear regression as: Here, Y= Dependent Variable (Target Variable)X= Independent Variable (predictor Variable)a0= intercept of the line (Gives an additional degree of freedom)a1 = Linear regression coefficient (scale factor to each input value). = random error The values for x and y variables are training datasets for Linear Regression model representation."
What are the assumptions about the form of the model?,ML_notes_csv,There are three stages to build hypotheses or model in machine learning: Model buildingIt chooses a suitable algorithm for the model and trains it according to the requirement of the problem.Applying the modelIt is responsible for checking the accuracy of the model through the test data.Model testingIt performs the required changes after testing and apply the final model.
What are the Assumptions about the residuals?,ML_notes_csv,"Below are some important assumptions of Linear Regression. These are some formal checks while building a Linear Regression model, which ensures to get the best possible result from the given dataset. Linear relationship between the features and target:Linear regression assumes the linear relationship between the dependent and independent variables.Small or no multicollinearity between the features:Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. Or we can say, it is difficult to determine which predictor variable is affecting the target variable and which is not. So, the model assumes either little or no multicollinearity between the features or independent variables.Homoscedasticity Assumption:Homoscedasticity is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.Normal distribution of error terms:Linear regression assumes that the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.It can be checked using theq-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.No autocorrelations:The linear regression model assumes no autocorrelation in error terms. If there will be any correlation in the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if there is a dependency between residual errors."
What are the assumptions about the estimators?,ML_notes_csv,"Below are some important assumptions of Linear Regression. These are some formal checks while building a Linear Regression model, which ensures to get the best possible result from the given dataset. Linear relationship between the features and target:Linear regression assumes the linear relationship between the dependent and independent variables.Small or no multicollinearity between the features:Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. Or we can say, it is difficult to determine which predictor variable is affecting the target variable and which is not. So, the model assumes either little or no multicollinearity between the features or independent variables.Homoscedasticity Assumption:Homoscedasticity is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.Normal distribution of error terms:Linear regression assumes that the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.It can be checked using theq-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.No autocorrelations:The linear regression model assumes no autocorrelation in error terms. If there will be any correlation in the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if there is a dependency between residual errors."
Explain the difference between Correlation and Regression.,ML_notes_csv,"Regression is a process of finding the correlations between dependent and independent variables. It helps in predicting the continuous variables such as prediction ofMarket Trends, prediction of House prices, etc. The task of the Regression algorithm is to find the mapping function to map the input variable(x) to the continuous output variable(y). Example:Suppose we want to do weather forecasting, so for this, we will use the Regression algorithm. In weather prediction, the model is trained on the past data, and once the training is completed, it can easily predict the weather for future days. Types of Regression Algorithm: Simple Linear RegressionMultiple Linear RegressionPolynomial RegressionSupport Vector RegressionDecision Tree RegressionRandom Forest Regression"
Explain the Gradient Descent algorithm with respect to linear regression.,ML_notes_csv,Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.A regression model uses gradient descent to update the coefficients of the line by reducing the cost function.It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.
Justify the cases where the linear regression algorithm is suitable for a given dataset.,ML_notes_csv,"Linear regression is a statistical regression method which is used for predictive analysis.It is one of the very simple and easy algorithms which works on regression and shows the relationship between the continuous variables.It is used for solving the regression problem in machine learning.Linear regression shows the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis), hence called linear regression.If there is only one input variable (x), then such linear regression is calledsimple linear regression. And if there is more than one input variable, then such linear regression is calledmultiple linear regression.The relationship between variables in the linear regression model can be explained using the below image. Here we are predicting the salary of an employee on the basis ofthe year of experience. Below is the mathematical equation for Linear regression: Here, Y = dependent variables (target variables),X= Independent variables (predictor variables),a and b are the linear coefficients Some popular applications of linear regression are: Analyzing trends and sales estimatesSalary forecastingReal estate predictionArriving at ETAs in traffic."
List down some of the metrics used to evaluate a Regression Model.,ML_notes_csv,"Once you have trained your model, it's time to assess its performance. There are various metrics used to evaluate model performance, categorized based on the type of task: regression/numerical or classification."
"For a linear regression model, how do we interpret a Q-Q plot?",ML_notes_csv,"The Simple Linear Regression model can be represented using the below equation: Where, a0= It is the intercept of the Regression line (can be obtained putting x=0)a1= It is the slope of the regression line, which tells whether the line is increasing or decreasing. = The error term. (For a good model it will be negligible)"
"In linear regression, what is the value of the sum of the residuals for a given dataset? Explain with proper justification.",ML_notes_csv,"The different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the cost function is used to estimate the values of the coefficient for the best fit line.Cost function optimizes the regression coefficients or weights. It measures how a linear regression model is performing.We can use the cost function to find the accuracy of themapping function, which maps the input variable to the output variable. This mapping function is also known asHypothesis function. For Linear Regression, we use theMean Squared Error (MSE)cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as: For the above linear equation, MSE can be calculated as: Where, N=Total number of observationYi = Actual value(a1xi+a0)= Predicted value. Residuals:The distance between the actual value and predicted values is called residual. If the observed points are far from the regression line, then the residual will be high, and so cost function will high. If the scatter points are close to the regression line, then the residual will be small and hence the cost function."
What are RMSE and MSE? How to calculate it?,ML_notes_csv,RMSE is a popular method and is the extended version of MSE(Mean Squared Error). This method is basically used to evaluate the performance of our model. It indicates how much the data points are spread around the best line. It is the standard deviation of the Mean squared error. A lower value means that the data point lies closer to the best fit line. Output:
What is OLS?,ML_notes_csv,"Support Vector Machine is a supervised learning algorithm which can be used for regression as well as classification problems. So if we use it for regression problems, then it is termed as Support Vector Regression. Support Vector Regression is a regression algorithm which works for continuous variables. Below are some keywords which are used inSupport Vector Regression: Kernel:It is a function used to map a lower-dimensional data into higher dimensional data.Hyperplane:In general SVM, it is a separation line between two classes, but in SVR, it is a line which helps to predict the continuous variables and cover most of the datapoints.Boundary line:Boundary lines are the two lines apart from hyperplane, which creates a margin for datapoints.Support vectors:Support vectors are the datapoints which are nearest to the hyperplane and opposite class. In SVR, we always try to determine a hyperplane with a maximum margin, so that maximum number of datapoints are covered in that margin.The main goal of SVR is to consider the maximum datapoints within the boundary lines and the hyperplane (best-fit line) must contain a maximum number of datapoints. Consider the below image: Here, the blue line is called hyperplane, and the other two lines are known as boundary lines."
What are MAE and MAPE?,ML_notes_csv,"This is the simplest metric used to analyze the loss over the whole dataset. As we all know the error is basically the difference between the predicted and actual values. Therefore MAE is defined as the average of the errors calculated. Here we calculate the modulus of the error, perform the summation and then divide the result by the number of data points. It is a positive quantity and is not concerned about the direction. The formula of MAE is given by Output:"
Why do we square the residuals instead of using modulus?,ML_notes_csv,"This is the simplest metric used to analyze the loss over the whole dataset. As we all know the error is basically the difference between the predicted and actual values. Therefore MAE is defined as the average of the errors calculated. Here we calculate the modulus of the error, perform the summation and then divide the result by the number of data points. It is a positive quantity and is not concerned about the direction. The formula of MAE is given by Output:"
List down the techniques that are adopted to find the parameters of the linear regression line which best fits the model.,ML_notes_csv,"When working with linear regression, our main goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error. The different values for weights or the coefficient of lines (a0, a1) gives a different line of regression, so we need to calculate the best values for a0and a1to find the best fit line, so to calculate this we use cost function."
Which evaluation metric should you prefer to use for a dataset having a lot of outliers in it?,ML_notes_csv,"Outliers are extreme values that deviate significantly from the majority of the data. They can negatively impact the analysis and model performance. Techniques such as clustering, interpolation, or transformation can be used to handle outliers. To check the outliers, We generally use a box plot. A box plot, also referred to as a box-and-whisker plot, is a graphical representation of a datasets distribution. It shows a variables median, quartiles, and potential outliers. The line inside the box denotes the median, while the box itself denotes the interquartile range (IQR). The whiskers extend to the most extreme non-outlier values within 1.5 times the IQR. Individual points beyond the whiskers are considered potential outliers. A box plot offers an easy-to-understand overview of the range of the data and makes it possible to identify outliers or skewness in the distribution. Lets plot the box plot for Age column data. Output: As we can see from the above Box and whisker plot, Our age dataset has outliers values. The values less than 5 and more than 55 are outliers. Output: Similarly, we can remove the outliers of the remaining columns."
Explain the normal form equation of the linear regression.,ML_notes_csv,"The Simple Linear Regression model can be represented using the below equation: Where, a0= It is the intercept of the Regression line (can be obtained putting x=0)a1= It is the slope of the regression line, which tells whether the line is increasing or decreasing. = The error term. (For a good model it will be negligible)"
When should it be preferred to the Gradient Descent method instead of the Normal Equation in Linear Regression Algorithm?,ML_notes_csv,Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.A regression model uses gradient descent to update the coefficients of the line by reducing the cost function.It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.
What are R-squared and Adjusted R-squared?,ML_notes_csv,"The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is calledoptimization. It can be achieved by below method: 1. R-squared method: R-squared is a statistical method that determines the goodness of fit.It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.It is also called acoefficient of determination,orcoefficient of multiple determinationfor multiple regression.It can be calculated from the below formula:"
What are the flaws in R-squared?,ML_notes_csv,"The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is calledoptimization. It can be achieved by below method: 1. R-squared method: R-squared is a statistical method that determines the goodness of fit.It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.It is also called acoefficient of determination,orcoefficient of multiple determinationfor multiple regression.It can be calculated from the below formula:"
What is Multicollinearity?,ML_notes_csv,The dependent variable must be categorical in nature.The independent variable should not have multi-collinearity.
What is Heteroscedasticity? How to detect it?,ML_notes_csv,"Network Latency: The communication between the devices and the central server can be a bottleneck and may add latency to the training process. Heterogeneous devices: The devices can be heterogeneous in terms of hardware and software, which can make it difficult to ensure the compatibility and consistency of the models. Data Quality: The quality of data can vary across the devices, which can lead to poor model performance."
What are the disadvantages of the linear regression Algorithm?,ML_notes_csv,There are mainly two applications of Multiple Linear Regression: Effectiveness of Independent variable on prediction:Predicting the impact of changes:
What is VIF? How do you calculate it?,ML_notes_csv,"The F1 score represents the measurement of a model's performance. It is referred to as a weighted average of the precision and recall of a model. The results tending to1are considered as the best, and those tending to0are the worst. It could be used in classification tests, where true negatives don't matter much."
Is it possible to apply Linear Regression for Time Series Analysis?,ML_notes_csv,"Linear regression is a statistical regression method which is used for predictive analysis.It is one of the very simple and easy algorithms which works on regression and shows the relationship between the continuous variables.It is used for solving the regression problem in machine learning.Linear regression shows the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis), hence called linear regression.If there is only one input variable (x), then such linear regression is calledsimple linear regression. And if there is more than one input variable, then such linear regression is calledmultiple linear regression.The relationship between variables in the linear regression model can be explained using the below image. Here we are predicting the salary of an employee on the basis ofthe year of experience. Below is the mathematical equation for Linear regression: Here, Y = dependent variables (target variables),X= Independent variables (predictor variables),a and b are the linear coefficients Some popular applications of linear regression are: Analyzing trends and sales estimatesSalary forecastingReal estate predictionArriving at ETAs in traffic."
What do you mean by the Logistic Regression?,ML_notes_csv,"Logistic regression is another supervised learning algorithm which is used to solve the classification problems. Inclassification problems, we have dependent variables in a binary or discrete format such as 0 or 1.Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, Spam or not spam, etc.It is a predictive analysis algorithm which works on the concept of probability.Logistic regression is a type of regression, but it is different from the linear regression algorithm in the term how they are used.Logistic regression usessigmoid functionor logistic function which is a complex cost function. This sigmoid function is used to model the data in logistic regression. The function can be represented as: f(x)= Output between the 0 and 1 value.x= input to the functione= base of natural logarithm. When we provide the input values (data) to the function, it gives the S-curve as follows: It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values below the threshold level are rounded up to 0. There are three types of logistic regression: Binary(0/1, pass/fail)Multi(cats, dogs, lions)Ordinal(low, medium, high)"
What are the different types of Logistic Regression?,ML_notes_csv,"On the basis of the categories, Logistic Regression can be classified into three types: Binomial:In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.Multinomial:In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as ""cat"", ""dogs"", or ""sheep""Ordinal:In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as ""low"", ""Medium"", or ""High""."
Explain the intuition behind Logistic Regression in detail.,ML_notes_csv,"Logistic regression is another supervised learning algorithm which is used to solve the classification problems. Inclassification problems, we have dependent variables in a binary or discrete format such as 0 or 1.Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, Spam or not spam, etc.It is a predictive analysis algorithm which works on the concept of probability.Logistic regression is a type of regression, but it is different from the linear regression algorithm in the term how they are used.Logistic regression usessigmoid functionor logistic function which is a complex cost function. This sigmoid function is used to model the data in logistic regression. The function can be represented as: f(x)= Output between the 0 and 1 value.x= input to the functione= base of natural logarithm. When we provide the input values (data) to the function, it gives the S-curve as follows: It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values below the threshold level are rounded up to 0. There are three types of logistic regression: Binary(0/1, pass/fail)Multi(cats, dogs, lions)Ordinal(low, medium, high)"
What are the odds?,ML_notes_csv,"Now we will check the accuracy of the Naive Bayes classifier using the Confusion matrix. Below is the code for it: Output: As we can see in the above confusion matrix output, there are 7+3= 10 incorrect predictions, and 65+25=90 correct predictions."
What factors can attribute to the popularity of Logistic Regression?,ML_notes_csv,"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an ""S"" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
Is the decision boundary Linear or Non-linear in the case of a Logistic Regression model?,ML_notes_csv,"The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below: We know the equation of the straight line can be written as: In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y): But we need range between -infinity to +infinity, then take logarithm of the equation it will become: The above equation is the final equation for Logistic Regression."
What is the Impact of Outliers on Logistic Regression?,ML_notes_csv,"Dependent Variable:The main factor in Regression analysis which we want to predict or understand is called the dependent variable. It is also calledtarget variable.Independent Variable:The factors which affect the dependent variables or which are used to predict the values of the dependent variables are called independent variable, also called as apredictor.Outliers:Outlier is an observation which contains either very low value or very high value in comparison to other observed values. An outlier may hamper the result, so it should be avoided.Multicollinearity:If the independent variables are highly correlated with each other than other variables, then such condition is called Multicollinearity. It should not be present in the dataset, because it creates problem while ranking the most affecting variable.Underfitting and Overfitting:If our algorithm works well with the training dataset but not well with test dataset, then such problem is calledOverfitting. And if our algorithm does not perform well even with training dataset, then such problem is calledunderfitting."
What is the difference between the outputs of the Logistic model and the Logistic function?,ML_notes_csv,"Logistic regression is another supervised learning algorithm which is used to solve the classification problems. Inclassification problems, we have dependent variables in a binary or discrete format such as 0 or 1.Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, Spam or not spam, etc.It is a predictive analysis algorithm which works on the concept of probability.Logistic regression is a type of regression, but it is different from the linear regression algorithm in the term how they are used.Logistic regression usessigmoid functionor logistic function which is a complex cost function. This sigmoid function is used to model the data in logistic regression. The function can be represented as: f(x)= Output between the 0 and 1 value.x= input to the functione= base of natural logarithm. When we provide the input values (data) to the function, it gives the S-curve as follows: It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values below the threshold level are rounded up to 0. There are three types of logistic regression: Binary(0/1, pass/fail)Multi(cats, dogs, lions)Ordinal(low, medium, high)"
How do we handle categorical variables in Logistic Regression?,ML_notes_csv,"On the basis of the categories, Logistic Regression can be classified into three types: Binomial:In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.Multinomial:In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as ""cat"", ""dogs"", or ""sheep""Ordinal:In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as ""low"", ""Medium"", or ""High""."
"Which algorithm is better in the case of outliers present in the dataset i.e., Logistic Regression or SVM?",ML_notes_csv,SVM stands forSupport Vector Machine. SVM are supervised learning models with an associated learning algorithm which analyze the data used for classification and regression analysis. The classification methods that SVM can handle are: Combining binary classifiersModifying binary to incorporate multiclass learning
What are the assumptions made in Logistic Regression?,ML_notes_csv,"The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below: We know the equation of the straight line can be written as: In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y): But we need range between -infinity to +infinity, then take logarithm of the equation it will become: The above equation is the final equation for Logistic Regression."
Can we solve the multiclass classification problems using Logistic Regression? If Yes then How?,ML_notes_csv,"On the basis of the categories, Logistic Regression can be classified into three types: Binomial:In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.Multinomial:In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as ""cat"", ""dogs"", or ""sheep""Ordinal:In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as ""low"", ""Medium"", or ""High""."
Discuss the space complexity of Logistic Regression.,ML_notes_csv,"Logistic regression is another supervised learning algorithm which is used to solve the classification problems. Inclassification problems, we have dependent variables in a binary or discrete format such as 0 or 1.Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, Spam or not spam, etc.It is a predictive analysis algorithm which works on the concept of probability.Logistic regression is a type of regression, but it is different from the linear regression algorithm in the term how they are used.Logistic regression usessigmoid functionor logistic function which is a complex cost function. This sigmoid function is used to model the data in logistic regression. The function can be represented as: f(x)= Output between the 0 and 1 value.x= input to the functione= base of natural logarithm. When we provide the input values (data) to the function, it gives the S-curve as follows: It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values below the threshold level are rounded up to 0. There are three types of logistic regression: Binary(0/1, pass/fail)Multi(cats, dogs, lions)Ordinal(low, medium, high)"
Discuss the Test or Runtime complexity of Logistic Regression.,ML_notes_csv,"To understand the implementation of Logistic Regression in Python, we will use the below example: Example:There is a dataset given which contains the information of various users obtained from the social networking sites. There is a car making company that has recently launched a new SUV car. So the company wanted to check how many users from the dataset, wants to purchase the car. For this problem, we will build a Machine Learning model using the Logistic regression algorithm. The dataset is shown in the below image. In this problem, we will predict thepurchased variable (Dependent Variable)by usingage and salary (Independent variables). Steps in Logistic Regression:To implement the Logistic Regression using Python, we will use the same steps as we have done in previous topics of Regression. Below are the steps: Data Pre-processing stepFitting Logistic Regression to the Training setPredicting the test resultTest accuracy of the result(Creation of Confusion matrix)Visualizing the test set result. 1. Data Pre-processing step:In this step, we will pre-process/prepare the data so that we can use it in our code efficiently. It will be the same as we have done in Data pre-processing topic. The code for this is given below: By executing the above lines of code, we will get the dataset as the output. Consider the given image: Now, we will extract the dependent and independent variables from the given dataset. Below is the code for it: In the above code, we have taken 2, 3 for x because our independent variables are age and salary, which are at index 2, 3. And we have taken 4 for y variable because our dependent variable is at index 4. The output will be: Now we will split the dataset into a training set and test set. Below is the code for it: The output for this is given below: For test set: For training set: In logistic regression, we will do feature scaling because we want accurate result of predictions. Here we will only scale the independent variable because dependent variable have only 0 and 1 values. Below is the code for it: The scaled output is given below: 2. Fitting Logistic Regression to the Training set: We have well prepared our dataset, and now we will train the dataset using the training set. For providing training or fitting the model to the training set, we will import theLogisticRegressionclass of thesklearnlibrary. After importing the class, we will create a classifier object and use it to fit the model to the logistic regression. Below is the code for it: Output:By executing the above code, we will get the below output: Out5: Hence our model is well fitted to the training set. 3. Predicting the Test Result Our model is well trained on the training set, so we will now predict the result by using test set data. Below is the code for it: In the above code, we have created a y_pred vector to predict the test set result. Output:By executing the above code, a new vector (y_pred) will be created under the variable explorer option. It can be seen as: The above output image shows the corresponding predicted users who want to purchase or not purchase the car. 4. Test Accuracy of the result Now we will create the confusion matrix here to check the accuracy of the classification. To create it, we need to import theconfusion_matrixfunction of the sklearn library. After importing the function, we will call it using a new variablecm. The function takes two parameters, mainlyy_true( the actual values) andy_pred(the targeted value return by the classifier). Below is the code for it: Output: By executing the above code, a new confusion matrix will be created. Consider the below image: We can find the accuracy of the predicted result by interpreting the confusion matrix. By above output, we can interpret that 65+24= 89 (Correct Output) and 8+3= 11(Incorrect Output). 5. Visualizing the training set result Finally, we will visualize the training set result. To visualize the result, we will useListedColormapclass of matplotlib library. Below is the code for it: In the above code, we have imported theListedColormapclass of Matplotlib library to create the colormap for visualizing the result. We have created two new variablesx_setandy_setto replacex_trainandy_train. After that, we have used thenm.meshgridcommand to create a rectangular grid, which has a range of -1(minimum) to 1 (maximum). The pixel points we have taken are of 0.01 resolution. To create a filled contour, we have usedmtp.contourfcommand, it will create regions of provided colors (purple and green). In this function, we have passed theclassifier.predictto show the predicted data points predicted by the classifier. Output:By executing the above code, we will get the below output: The graph can be explained in the below points: In the above graph, we can see that there are someGreen pointswithin the green region andPurple pointswithin the purple region.All these data points are the observation points from the training set, which shows the result for purchased variables.This graph is made by using two independent variables i.e.,Age on the x-axisandEstimated salary on the y-axis.Thepurple point observationsare for which purchased (dependent variable) is probably 0, i.e., users who did not purchase the SUV car.Thegreen point observationsare for which purchased (dependent variable) is probably 1 means user who purchased the SUV car.We can also estimate from the graph that the users who are younger with low salary, did not purchase the car, whereas older users with high estimated salary purchased the car.But there are some purple points in the green region (Buying the car) and some green points in the purple region(Not buying the car). So we can say that younger users with a high estimated salary purchased the car, whereas an older user with a low estimated salary did not purchase the car. The goal of the classifier: We have successfully visualized the training set result for the logistic regression, and our goal for this classification is to divide the users who purchased the SUV car and who did not purchase the car. So from the output graph, we can clearly see the two regions (Purple and Green) with the observation points. The Purple region is for those users who didn't buy the car, and Green Region is for those users who purchased the car. Linear Classifier: As we can see from the graph, the classifier is a Straight line or linear in nature as we have used the Linear model for Logistic Regression. In further topics, we will learn for non-linear Classifiers. Visualizing the test set result: Our model is well trained using the training dataset. Now, we will visualize the result for new observations (Test set). The code for the test set will remain same as above except that here we will usex_test and y_testinstead ofx_train and y_train. Below is the code for it: Output: The above graph shows the test set result. As we can see, the graph is divided into two regions (Purple and Green). And Green observations are in the green region, and Purple observations are in the purple region. So we can say it is a good prediction and model. Some of the green and purple data points are in different regions, which can be ignored as we have already calculated this error using the confusion matrix (11 Incorrect output). Hence our model is pretty good and ready to make new predictions for this classification problem."
Why is Logistic Regression termed as Regression and not classification?,ML_notes_csv,"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an ""S"" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
Discuss the Train complexity of Logistic Regression.,ML_notes_csv,The possibility of overfitting occurs when the criteria used for training the model is not as per the criteria used to judge the efficiency of a model.
Why can’t we use Mean Square Error (MSE) as a cost function for Logistic Regression?,ML_notes_csv,Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.A regression model uses gradient descent to update the coefficients of the line by reducing the cost function.It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.
Why can’t we use Linear Regression in place of Logistic Regression for Binary classification?,ML_notes_csv,"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an ""S"" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
What are the advantages of Logistic Regression?,ML_notes_csv,"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an ""S"" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
. What are the disadvantages of Logistic Regression?,ML_notes_csv,"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1,it gives the probabilistic values which lie between 0 and 1.Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereasLogistic regression is used for solving the classification problems.In Logistic regression, instead of fitting a regression line, we fit an ""S"" shaped logistic function, which predicts two maximum values (0 or 1).The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
What are Support Vector Machines (SVMs)?,ML_notes_csv,SVM stands forSupport Vector Machine. SVM are supervised learning models with an associated learning algorithm which analyze the data used for classification and regression analysis. The classification methods that SVM can handle are: Combining binary classifiersModifying binary to incorporate multiclass learning
What are Support Vectors in SVMs?,ML_notes_csv,"Support Vector Machine is a supervised learning algorithm which can be used for regression as well as classification problems. So if we use it for regression problems, then it is termed as Support Vector Regression. Support Vector Regression is a regression algorithm which works for continuous variables. Below are some keywords which are used inSupport Vector Regression: Kernel:It is a function used to map a lower-dimensional data into higher dimensional data.Hyperplane:In general SVM, it is a separation line between two classes, but in SVR, it is a line which helps to predict the continuous variables and cover most of the datapoints.Boundary line:Boundary lines are the two lines apart from hyperplane, which creates a margin for datapoints.Support vectors:Support vectors are the datapoints which are nearest to the hyperplane and opposite class. In SVR, we always try to determine a hyperplane with a maximum margin, so that maximum number of datapoints are covered in that margin.The main goal of SVR is to consider the maximum datapoints within the boundary lines and the hyperplane (best-fit line) must contain a maximum number of datapoints. Consider the below image: Here, the blue line is called hyperplane, and the other two lines are known as boundary lines."
What is the basic principle of a Support Vector Machine?,ML_notes_csv,"Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane: Example:SVM can be understood with the example that we have used in the KNN classifier. Suppose we see a strange cat that also has some features of dogs, so if we want a model that can accurately identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will first train our model with lots of images of cats and dogs so that it can learn about different features of cats and dogs, and then we test it with this strange creature. So as support vector creates a decision boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see the extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider the below diagram: SVM algorithm can be used forFace detection, image classification, text categorization,etc."
What are hard margin and soft Margin SVMs?,ML_notes_csv,"Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane: Example:SVM can be understood with the example that we have used in the KNN classifier. Suppose we see a strange cat that also has some features of dogs, so if we want a model that can accurately identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will first train our model with lots of images of cats and dogs so that it can learn about different features of cats and dogs, and then we test it with this strange creature. So as support vector creates a decision boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see the extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider the below diagram: SVM algorithm can be used forFace detection, image classification, text categorization,etc."
What do you mean by Hinge loss?,ML_notes_csv,"This is the simplest metric used to analyze the loss over the whole dataset. As we all know the error is basically the difference between the predicted and actual values. Therefore MAE is defined as the average of the errors calculated. Here we calculate the modulus of the error, perform the summation and then divide the result by the number of data points. It is a positive quantity and is not concerned about the direction. The formula of MAE is given by Output:"
What is the “Kernel trick”?,ML_notes_csv,"In machine learning,lazy learningcan be described as a method where induction and generalization processes are delayed until classification is performed. Because of the same property, an instance-based learning algorithm is sometimes called lazy learning algorithm."
What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?,ML_notes_csv,"Hyperplane:There can be multiple lines/decision boundaries to segregate the classes in n-dimensional space, but we need to find out the best decision boundary that helps to classify the data points. This best boundary is known as the hyperplane of SVM. The dimensions of the hyperplane depend on the features present in the dataset, which means if there are 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, then hyperplane will be a 2-dimension plane. We always create a hyperplane that has a maximum margin, which means the maximum distance between the data points. Support Vectors: The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector."
Explain different types of kernel functions.,ML_notes_csv,"Classification is a process of finding a function which helps in dividing the dataset into classes based on different parameters. In Classification, a computer program is trained on the training dataset and based on that training, it categorizes the data into different classes. The task of the classification algorithm is to find the mapping function to map the input(x) to the discrete output(y). Example:The best example to understand the Classification problem is Email Spam Detection. The model is trained on the basis of millions of emails on different parameters, and whenever it receives a new email, it identifies whether the email is spam or not. If the email is spam, then it is moved to the Spam folder. Types of ML Classification Algorithms: Classification Algorithms can be further divided into the following types: Logistic RegressionK-Nearest NeighboursSupport Vector MachinesKernel SVMNave BayesDecision Tree ClassificationRandom Forest Classification"
How you formulate SVM for a regression problem statement?,ML_notes_csv,SVM stands forSupport Vector Machine. SVM are supervised learning models with an associated learning algorithm which analyze the data used for classification and regression analysis. The classification methods that SVM can handle are: Combining binary classifiersModifying binary to incorporate multiclass learning
What affects the decision boundary in SVM?,ML_notes_csv,"Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane: Example:SVM can be understood with the example that we have used in the KNN classifier. Suppose we see a strange cat that also has some features of dogs, so if we want a model that can accurately identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will first train our model with lots of images of cats and dogs so that it can learn about different features of cats and dogs, and then we test it with this strange creature. So as support vector creates a decision boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see the extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider the below diagram: SVM algorithm can be used forFace detection, image classification, text categorization,etc."
What is a slack variable?,ML_notes_csv,"Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression. The key point in Simple Linear Regression is that thedependent variable must be a continuous/real value. However, the independent variable can be measured on continuous or categorical values. Simple Linear regression algorithm has mainly two objectives: Model the relationship between the two variables.Such as the relationship between Income and expenditure, experience and Salary, etc.Forecasting new observations.Such as Weather forecasting according to temperature, Revenue of a company according to the investments in a year, etc."
What is a dual and primal problem and how is it relevant to SVMs?,ML_notes_csv,"Linear SVM: The working of the SVM algorithm can be understood by using an example. Suppose we have a dataset that has two tags (green and blue), and the dataset has two features x1 and x2. We want a classifier that can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image: So as it is 2-d space so by just using a straight line, we can easily separate these two classes. But there can be multiple lines that can separate these classes. Consider the below image: Hence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region is called as ahyperplane. SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called asmargin. And the goal of SVM is to maximize this margin. Thehyperplanewith maximum margin is called theoptimal hyperplane. Non-Linear SVM: If data is linearly arranged, then we can separate it by using a straight line, but for non-linear data, we cannot draw a single straight line. Consider the below image: So to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions x and y, so for non-linear data, we will add a third dimension z. It can be calculated as: By adding the third dimension, the sample space will become as below image: So now, SVM will divide the datasets into classes in the following way. Consider the below image: Since we are in 3-d Space, hence it is looking like a plane parallel to the x-axis. If we convert it in 2d space with z=1, then it will become as: Hence we get a circumference of radius 1 in case of non-linear data. Python Implementation of Support Vector Machine Now we will implement the SVM algorithm using Python. Here we will use the same datasetuser_data, which we have used in Logistic regression and KNN classification. Data Pre-processing step Till the Data pre-processing step, the code will remain the same. Below is the code: After executing the above code, we will pre-process the data. The code will give the dataset as: The scaled output for the test set will be: Fitting the SVM classifier to the training set: Now the training set will be fitted to the SVM classifier. To create the SVM classifier, we will importSVCclass fromSklearn.svmlibrary. Below is the code for it: In the above code, we have usedkernel='linear', as here we are creating SVM for linearly separable data. However, we can change it for non-linear data. And then we fitted the classifier to the training dataset(x_train, y_train) Output: The model performance can be altered by changing the value ofC(Regularization factor), gamma, and kernel. Predicting the test set result:Now, we will predict the output for test set. For this, we will create a new vector y_pred. Below is the code for it: After getting the y_pred vector, we can compare the result ofy_predandy_testto check the difference between the actual value and predicted value. Output:Below is the output for the prediction of the test set: Creating the confusion matrix:Now we will see the performance of the SVM classifier that how many incorrect predictions are there as compared to the Logistic regression classifier. To create the confusion matrix, we need to import theconfusion_matrixfunction of the sklearn library. After importing the function, we will call it using a new variablecm. The function takes two parameters, mainlyy_true( the actual values) andy_pred(the targeted value return by the classifier). Below is the code for it: Output: As we can see in the above output image, there are 66+24= 90 correct predictions and 8+2= 10 correct predictions. Therefore we can say that our SVM model improved as compared to the Logistic regression model. Visualizing the training set result:Now we will visualize the training set result, below is the code for it: Output: By executing the above code, we will get the output as: As we can see, the above output is appearing similar to the Logistic regression output. In the output, we got the straight line as hyperplane because we haveused a linear kernel in the classifier. And we have also discussed above that for the 2d space, the hyperplane in SVM is a straight line. Visualizing the test set result: Output: By executing the above code, we will get the output as: As we can see in the above output image, the SVM classifier has divided the users into two regions (Purchased or Not purchased). Users who purchased the SUV are in the red region with the red scatter points. And users who did not purchase the SUV are in the green region with green scatter points. The hyperplane has divided the two classes into Purchased and not purchased variable."
Can an SVM classifier outputs a confidence score when it classifies an instance? What about a probability?,ML_notes_csv,"A classifier is a case of a hypothesis or discrete-valued function which is used to assign class labels to particular data points. It is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class."
If you train an SVM classifier with an RBF kernel. It seems to underfit the training dataset: should you increase or decrease the hyper-parameter γ (gamma)? What about the C hyper-parameter?,ML_notes_csv,"Hyperplane:There can be multiple lines/decision boundaries to segregate the classes in n-dimensional space, but we need to find out the best decision boundary that helps to classify the data points. This best boundary is known as the hyperplane of SVM. The dimensions of the hyperplane depend on the features present in the dataset, which means if there are 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, then hyperplane will be a 2-dimension plane. We always create a hyperplane that has a maximum margin, which means the maximum distance between the data points. Support Vectors: The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector."
Is SVM sensitive to the Feature Scaling?,ML_notes_csv,"Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Consider the below dataset:"
What is the basic assumption in the case of the Naive Bayes classifier?,ML_notes_csv,"The Naive Bayes algorithm is comprised of two words Naive and Bayes, Which can be described as: Naive: It is called Naive because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.Bayes: It is called Bayes because it depends on the principle ofBayes' Theorem."
What are the possible advantages of choosing the Naive Bayes classifier?,ML_notes_csv,Naive Bayes is one of the fast and easy ML algorithms to predict a class of datasets.It can be used for Binary as well as Multi-class Classifications.It performs well in Multi-class predictions as compared to the other Algorithms.It is the most popular choice fortext classification problems.
What disadvantages of Naive Bayes can make you remove it from your analysis?,ML_notes_csv,"Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features."
Is feature scaling required in Naive Bayes?,ML_notes_csv,"Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features."
Impact of missing values on naive Bayes?,ML_notes_csv,"Missing data is a common issue in real-world datasets, and it can occur due to various reasons such as human errors, system failures, or data collection issues. Various techniques can be used to handle missing data, such as imputation, deletion, or substitution. Lets check the % missing values columns-wise for each row using df.isnull() it checks whether the values are null or not and gives returns boolean values. and .sum() will sum the total number of null values rows and we divide it by the total number of rows present in the dataset then we multiply to get values in % i.e per 100 values how much values are null. Output: We cannot just ignore or remove the missing observation. They must be handled carefully as they can be an indication of something important. Dropping Observations with missing values.The fact that the value was missing may be informative in itself.Plus, in the real world, you often need to make predictions on new data even if some of the features are missing! The fact that the value was missing may be informative in itself. Plus, in the real world, you often need to make predictions on new data even if some of the features are missing! As we can see from the above result that Cabin has 77% null values and Age has 19.87% and Embarked has 0.22% of null values. So, its not a good idea to fill 77% of null values. So, we will drop the Cabin column. Embarked column has only 0.22% of null values so, we drop the null values rows of Embarked column. Output: Imputing the missing values from past observations.Again, missingness is almost always informative in itself, and you should tell your algorithm if a value was missing.Even if you build a model to impute your values, youre not adding any real information. Youre just reinforcing the patterns already provided by other features. Again, missingness is almost always informative in itself, and you should tell your algorithm if a value was missing. Even if you build a model to impute your values, youre not adding any real information. Youre just reinforcing the patterns already provided by other features. We can use Mean imputation or Median imputations for the case. Note: Mean imputation is suitable when the data is normally distributed and has no extreme outliers. Median imputation is preferable when the data contains outliers or is skewed. Output:"
Impact of outliers?,ML_notes_csv,"Outliers are extreme values that deviate significantly from the majority of the data. They can negatively impact the analysis and model performance. Techniques such as clustering, interpolation, or transformation can be used to handle outliers. To check the outliers, We generally use a box plot. A box plot, also referred to as a box-and-whisker plot, is a graphical representation of a datasets distribution. It shows a variables median, quartiles, and potential outliers. The line inside the box denotes the median, while the box itself denotes the interquartile range (IQR). The whiskers extend to the most extreme non-outlier values within 1.5 times the IQR. Individual points beyond the whiskers are considered potential outliers. A box plot offers an easy-to-understand overview of the range of the data and makes it possible to identify outliers or skewness in the distribution. Lets plot the box plot for Age column data. Output: As we can see from the above Box and whisker plot, Our age dataset has outliers values. The values less than 5 and more than 55 are outliers. Output: Similarly, we can remove the outliers of the remaining columns."
What are different problem statements you can solve using Naive Bayes?,ML_notes_csv,"The Naive Bayes algorithm is comprised of two words Naive and Bayes, Which can be described as: Naive: It is called Naive because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.Bayes: It is called Bayes because it depends on the principle ofBayes' Theorem."
Does Naive Bayes fall under the category of the discriminative or generative classifier?,ML_notes_csv,"Naive Bayes algorithm is a supervised learning algorithm, which is based onBayes theoremand used for solving classification problems.It is mainly used intext classificationthat includes a high-dimensional training dataset.Naive Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.Some popular examples of Naive Bayes Algorithm arespam filtration, Sentimental analysis, and classifying articles."
What do you know about posterior and prior probability in Naive Bayes,ML_notes_csv,"Naive Bayes algorithm is a supervised learning algorithm, which is based onBayes theoremand used for solving classification problems.It is mainly used intext classificationthat includes a high-dimensional training dataset.Naive Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.Some popular examples of Naive Bayes Algorithm arespam filtration, Sentimental analysis, and classifying articles."
How does Naive Bayes treats categorical and numerical values?,ML_notes_csv,Naive Bayes is one of the fast and easy ML algorithms to predict a class of datasets.It can be used for Binary as well as Multi-class Classifications.It performs well in Multi-class predictions as compared to the other Algorithms.It is the most popular choice fortext classification problems.
What is k-NN Algorithm?,ML_notes_csv,"K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.K-NN is anon-parametric algorithm, which means it does not make any assumption on underlying data.It is also called alazy learner algorithmbecause it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.Example:Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
What is the role of the k value in the k-NN algorithm?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
Why is k-NN a non-parametric algorithm?,ML_notes_csv,"K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.K-NN is anon-parametric algorithm, which means it does not make any assumption on underlying data.It is also called alazy learner algorithmbecause it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.Example:Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
Why is the odd value of ‘k’ preferred over an even value in the k-NN algorithm?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
How does the k-NN algorithm make predictions on unseen datasets?,ML_notes_csv,"K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.K-NN is anon-parametric algorithm, which means it does not make any assumption on underlying data.It is also called alazy learner algorithmbecause it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.Example:Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
Is Feature scaling required for the k-NN?,ML_notes_csv,"Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Consider the below dataset:"
Describe the method used for feature scaling in k-NN algorithm?,ML_notes_csv,"Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Consider the below dataset:"
What is the space and time complexity of the k-NN Algorithm?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
Can k-NN algorithm be used for a regression problem?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
Why is it recommended not to use the k-NN Algorithm for large datasets?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
How to choose the optimal value of k in the k-NN Algorithm?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
How to handle categorical variables in the k-NN Algorithm?,ML_notes_csv,"Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:"
How can you relate k-NN algorithm to the bias-variance Tradeoff?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
How do you handle missing data in KNN algorithm?,ML_notes_csv,"Missing data is one of the standard factors while working with data and handling. It is considered as one of the greatest challenges faced by the data analysts. There are many ways one can impute the missing values. Some of the common methods to handle missing data in datasets can be defined asdeleting the rows, replacing with mean/median/mode, predicting the missing values, assigning a unique category, using algorithms that support missing values, etc."
How does the curse of dimensionality affect KNN algorithm?,ML_notes_csv,"KNN or K nearest neighbors is a supervised algorithm which is used for classification purpose. In KNN, a test sample is given as the class of the majority of its nearest neighbors. On the other side, K-means is an unsupervised algorithm which is mainly used for clustering. In k-means clustering, it needs a set of unlabeled points and a threshold only. The algorithm further takes unlabeled data and learns how to cluster it into groups by computing the mean of the distance between different unlabeled points."
What are some of the applications of KNN algorithm?,ML_notes_csv,"KNN or K nearest neighbors is a supervised algorithm which is used for classification purpose. In KNN, a test sample is given as the class of the majority of its nearest neighbors. On the other side, K-means is an unsupervised algorithm which is mainly used for clustering. In k-means clustering, it needs a set of unlabeled points and a threshold only. The algorithm further takes unlabeled data and learns how to cluster it into groups by computing the mean of the distance between different unlabeled points."
What is a decision tree?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What are some advantages of using a decision tree?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What is entropy in the context of decision trees?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
List down some popular algorithms used for deriving Decision Trees and their attribute selection measures.,ML_notes_csv,Five popular algorithms are: Decision TreesProbabilistic NetworksNeural NetworksSupport Vector MachinesNearest Neighbor
Explain the CART Algorithm for Decision Trees.,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
List down the attribute selection measures used by the ID3 algorithm to construct a Decision Tree.,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
Briefly explain the properties of Gini Impurity.,ML_notes_csv,"AUC (Area Under Curve) is an evaluation metric that is used to analyze the classification model at different threshold values. The Receiver Operating Characteristic(ROC) curve is a probabilistic curve used to highlight the models performance. The curve has two parameters: TPR: It stands for True positive rate. It basically follows the formula of Recall. FPR: It stands for False Positive rate. It is defined as the ratio of False positives to the summation of false positives and True negatives. This curve is useful as it helps us to determine the models capacity to distinguish between different classes. Let us illustrate this with the help of a simple Python example Output: AUC score is a useful metric to evaluate the model. It basically highlights a models capacity to separate the classes. In the above code, 0.75 is a good AUC score. A model is considered good if the AUC score is greater than 0.5 and approaches 1. A poor model has an AUC score of 0."
Explain the difference between the CART and ID3 Algorithms.,ML_notes_csv,"In the automotive industry, ML is the backbone of autonomous vehicles. These self-driving cars use ML to process data from sensors and make real-time decisions, promising safer and more efficient transportation."
Which should be preferred among Gini impurity and Entropy?,ML_notes_csv,"Once our model is completed, it is necessary to evaluate its performance; either it is a Classification or Regression model. So for evaluating a Classification model, we have the following ways: 1. Log Loss or Cross-Entropy Loss: It is used for evaluating the performance of a classifier, whose output is a probability value between the 0 and 1.For a good binary Classification model, the value of log loss should be near to 0.The value of log loss increases if the predicted value deviates from the actual value.The lower log loss represents the higher accuracy of the model.For Binary classification, cross-entropy can be calculated as: Where y= Actual output, p= predicted output. 2. Confusion Matrix: The confusion matrix provides us a matrix/table as output and describes the performance of the model.It is also known as the error matrix.The matrix consists of predictions result in a summarized form, which has a total number of correct predictions and incorrect predictions. The matrix looks like as below table: 3. AUC-ROC curve: ROC curve stands forReceiver Operating Characteristics Curveand AUC stands forArea Under the Curve.It is a graph that shows the performance of the classification model at different thresholds.To visualize the performance of the multi-class classification model, we use the AUC-ROC Curve.The ROC curve is plotted with TPR and FPR, where TPR (True Positive Rate) on Y-axis and FPR(False Positive Rate) on X-axis."
List down the different types of nodes in Decision Trees.,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
"What do you understand about Information Gain? Also, explain the mathematical formulation associated with it.",ML_notes_csv,"In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role ofMachine Learning. Introduction to Machine Learning A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. Arthur Samuel first used the term ""machine learning"" in 1959. It could be summarized as follows: Without being explicitly programmed, machine learning enables a machine to automatically learn from data, improve performance from experiences, and predict things. Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data. For the purpose of developing predictive models, machine learning brings together statistics and computer science. Algorithms that learn from historical data are either constructed or utilized in machine learning. The performance will rise in proportion to the quantity of information we provide. A machine can learn if it can gain more data to improve its performance."
Do we require Feature Scaling for Decision Trees? Explain,ML_notes_csv,"Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Consider the below dataset:"
What are the disadvantages of Information Gain?,ML_notes_csv,"ML applications can raise ethical issues, particularly concerning privacy and bias. Data privacy is a significant concern, as ML models often require access to sensitive and personal information. Bias in training data can lead to biased models, perpetuating existing inequalities and unfair treatment of certain groups."
List down the problem domains in which Decision Trees are most suitable.,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
Explain the time and space complexity of training and testing in the case of a Decision Tree.,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
"If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?",ML_notes_csv,Always needs to determine the value of K which may be complex some time.The computation cost is high because of calculating the distance between the data points for all the training samples.
How does a Decision Tree handle missing attribute values?,ML_notes_csv,"Missing data is one of the standard factors while working with data and handling. It is considered as one of the greatest challenges faced by the data analysts. There are many ways one can impute the missing values. Some of the common methods to handle missing data in datasets can be defined asdeleting the rows, replacing with mean/median/mode, predicting the missing values, assigning a unique category, using algorithms that support missing values, etc."
How does a Decision Tree handle continuous(numerical) features?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What is the Inductive Bias of Decision Trees?,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
Explain Feature Selection using the Information Gain/Entropy Technique.,ML_notes_csv,"As we have trained our model, our next step is to optimize our model more. Tuning and optimizing helps our model to maximize its performance and generalization ability. This process involves fine-tuning hyperparameters, selecting the best algorithm, and improving features through feature engineering techniques. Hyperparameters are parameters that are set before the training process begins and control the behavior of the machine learning model. These are like learning rate, regularization and parameters of the model should be carefully adjusted. Techniques like grid search cv randomized search and cross-validation are some optimization techniques that are used to systematically explore the hyperparameter space and identify the best combination of hyperparameters for the model. Overall, tuning and optimizing the model involves a combination of careful speculation of parameters, feature engineering, and other techniques to create a highly generalized model."
Compare the different attribute selection measures.,ML_notes_csv,"Accuracy: Proportion of correctly classified instances out of the total instances. Precision: Proportion of true positive predictions among all positive predictions. Recall: Proportion of true positive predictions among all actual positive instances. F1-score: Harmonic mean of precision and recall, providing a balanced measure of model performance. Area Under the Receiver Operating Characteristic curve (AUC-ROC): Measure of the model's ability to distinguish between classes. Confusion Metrics: It is a matrix that summarizes the performance of a classification model, showing counts of true positives, true negatives, false positives, and false negatives instances."
Is the Gini Impurity of a node lower or greater than that of its parent? Comment whether it is generally lower/greater or always lower/greater.,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
Why do we require Pruning in Decision Trees? Explain.,ML_notes_csv,"Pruning is said to occur in decision trees when the branches which may consist of weak predictive power are removed to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can occur bottom-up and top-down, with approaches such asreduced error pruningandcost complexity pruning. Reduced error pruning is the simplest version, and it replaces each node. If it is unable to decrease predictive accuracy, one should keep it pruned. But, it usually comes pretty close to an approach that would optimize for maximum accuracy."
Are Decision Trees affected by the outliers? Explain.,ML_notes_csv,"Unsupervised learning can also be applied to find those data points which greatly differ than the majorities. The statistics model may identify these outliers, or anomalies as signaling of errors, fraud or even something unusual. Local Outlier Factor (LOF) makes a comparison of a given data point's local density with those surrounding it. It then flags out the data points with significantly lower densities as outliers or potential anomalies. Isolation Forest is the one which uses different approach, which is to recursively isolate data points according to their features. Anomalies usually are simple to contemplate as they often necessitate fewer steps than an average normal point."
What do you understand by Pruning in a Decision Tree?,ML_notes_csv,"Pruning is said to occur in decision trees when the branches which may consist of weak predictive power are removed to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can occur bottom-up and top-down, with approaches such asreduced error pruningandcost complexity pruning. Reduced error pruning is the simplest version, and it replaces each node. If it is unable to decrease predictive accuracy, one should keep it pruned. But, it usually comes pretty close to an approach that would optimize for maximum accuracy."
List down the advantages of the Decision Trees.,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
List out the disadvantages of the Decision Trees.,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What is the role of decision trees in artificial intelligence and machine learning?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
How does the decision tree compare with linear regression and logistic regression?,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
What are the trade-offs of using decision trees vs. neural networks for certain machine learning applications?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What are some of the alternative algorithms to decision trees?,ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
"What common mistakes do beginners make when working with decision trees, and how can you avoid them?",ML_notes_csv,"Decision Trees can be defined as the Supervised Machine Learning, where the data is continuously split according to a certain parameter. It builds classification or regression models as similar as a tree structure, with datasets broken up into ever smaller subsets while developing the decision tree. The tree can be defined by two entities, namelydecision nodes, andleaves. The leaves are the decisions or the outcomes, and the decision nodes are where the data is split. Decision trees can manage both categorical and numerical data."
What is Bagging and How Does it Work? Explain it with Examples.,ML_notes_csv,Bagging is a process in ensemble learning which is used for improving unstable estimation or classification schemes.Boosting methods are used sequentially to reduce the bias of the combined model.
How is Bagging Different from the Random Forest Algorithm?,ML_notes_csv,"Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it."
What is the Difference Between Bootstrapping and Pasting in Bagging?,ML_notes_csv,"Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it."
Why does Bagging Performs Well on the Low Bias High Variance Datasets?,ML_notes_csv,Bagging is a process in ensemble learning which is used for improving unstable estimation or classification schemes.Boosting methods are used sequentially to reduce the bias of the combined model.
What is the Difference between Bagging and Boosting? Which is Better?,ML_notes_csv,"Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it."
What do you mean by Random Forest Algorithm?,ML_notes_csv,"Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model."
Why is Random Forest Algorithm popular?,ML_notes_csv,"Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model."
Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?,ML_notes_csv,"For a better predictive model, the categorical variable can be considered as a continuous variable only when the variable is ordinal in nature."
What do you mean by Bagging?,ML_notes_csv,Bagging is a process in ensemble learning which is used for improving unstable estimation or classification schemes.Boosting methods are used sequentially to reduce the bias of the combined model.
Explain the working of the Random Forest Algorithm.,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
Why do we prefer a Forest (collection of Trees) rather than a single Tree?,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
What do you mean by Bootstrap Sample?,ML_notes_csv,"Cluster Sampling is a process of randomly selecting intact groups within a defined population, sharing similar characteristics. Cluster sample is a probability where each sampling unit is a collection or cluster of elements. For example, if we are clustering the total number of managers in a set of companies, in that case, managers (sample) will represent elements and companies will represent clusters."
What is Out-of-Bag Error?,ML_notes_csv,This process occurs when data is unable to establish an accurate relationship between input and output variables. It simply means trying to fit in undersized jeans. It signifies the data is too simple to establish a precise relationship. To overcome this issue: Maximize the training time Enhance the complexity of the model Add more features to the data Reduce regular parameters Increasing the training time of model
What does random refer to in ‘Random Forest’?,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
Why does the Random Forest algorithm not require split sampling methods?,ML_notes_csv,"Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model."
List down the features of Bagged Trees,ML_notes_csv,Classification Algorithms can be further divided into the Mainly two category: Linear ModelsLogistic RegressionSupport Vector MachinesNon-linear ModelsK-Nearest NeighboursKernel SVMNave BayesDecision Tree ClassificationRandom Forest Classification
What are the Limitations of Bagging Trees?,ML_notes_csv,"Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it."
List down the factors on which the forest error rate depends upon.,ML_notes_csv,"Unsupervised learning can also be applied to find those data points which greatly differ than the majorities. The statistics model may identify these outliers, or anomalies as signaling of errors, fraud or even something unusual. Local Outlier Factor (LOF) makes a comparison of a given data point's local density with those surrounding it. It then flags out the data points with significantly lower densities as outliers or potential anomalies. Isolation Forest is the one which uses different approach, which is to recursively isolate data points according to their features. Anomalies usually are simple to contemplate as they often necessitate fewer steps than an average normal point."
How does a Random Forest Algorithm give predictions on an unseen dataset?,ML_notes_csv,"The classifier algorithms are designed to indicate whether a new data point belongs to one or another among several predefined classes. Imagine when you are organising emails into spam or inbox, categorising images as cat or dog, or predicting whether a loan applicant is a credible borrower. In the classification models, there is a learning process by the use of labeled examples from each category. In this process, they discover the correlations and relations within the data that help to distinguish class one from the other classes. After learning these patterns, the model is then capable of assigning these class labels to unseen data points. Common Classification Algorithms: Logistic Regression: A very efficient technique for the classification problems of binary nature (two types, for example, spam/not spam). Support Vector Machine (SVM): Good for tasks like classification, especially when the data has a large number of features. Decision Tree: Constructs a decision tree having branches and proceeds to the class predictions through features. Random Forest: The model generates an ""ensemble"" of decision trees that ultimately raise the accuracy and avoid overfitting (meaning that the model performs great on the training data but lousily on unseen data). K-Nearest Neighbors (KNN): Assigns a label of the nearest neighbors for a given data point."
Prove that in the Bagging method only about 63% of the total original examples (total training set) appear in any of sampled bootstrap datasets. Provide proper justification.,ML_notes_csv,"Similarities of Bagging and Boosting Both are the ensemble methods to get N learns from 1 learner.Both generate several training data sets with random sampling.Both generate the final result by taking the average of N learners.Both reduce variance and provide higher scalability. Differences between Bagging and Boosting Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it."
How to determine the overall OOB score for the classification problem statements in a Random Forest Algorithm?,ML_notes_csv,"Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model."
How does random forest define the Proximity (Similarity) between observations?,ML_notes_csv,"Unsupervised learning can also be applied to find those data points which greatly differ than the majorities. The statistics model may identify these outliers, or anomalies as signaling of errors, fraud or even something unusual. Local Outlier Factor (LOF) makes a comparison of a given data point's local density with those surrounding it. It then flags out the data points with significantly lower densities as outliers or potential anomalies. Isolation Forest is the one which uses different approach, which is to recursively isolate data points according to their features. Anomalies usually are simple to contemplate as they often necessitate fewer steps than an average normal point."
What is the use of proximity matrix in the random forest algorithm?,ML_notes_csv,"Unsupervised learning can also be applied to find those data points which greatly differ than the majorities. The statistics model may identify these outliers, or anomalies as signaling of errors, fraud or even something unusual. Local Outlier Factor (LOF) makes a comparison of a given data point's local density with those surrounding it. It then flags out the data points with significantly lower densities as outliers or potential anomalies. Isolation Forest is the one which uses different approach, which is to recursively isolate data points according to their features. Anomalies usually are simple to contemplate as they often necessitate fewer steps than an average normal point."
List down the parameters used to fine-tune the Random Forest.,ML_notes_csv,"As we have trained our model, our next step is to optimize our model more. Tuning and optimizing helps our model to maximize its performance and generalization ability. This process involves fine-tuning hyperparameters, selecting the best algorithm, and improving features through feature engineering techniques. Hyperparameters are parameters that are set before the training process begins and control the behavior of the machine learning model. These are like learning rate, regularization and parameters of the model should be carefully adjusted. Techniques like grid search cv randomized search and cross-validation are some optimization techniques that are used to systematically explore the hyperparameter space and identify the best combination of hyperparameters for the model. Overall, tuning and optimizing the model involves a combination of careful speculation of parameters, feature engineering, and other techniques to create a highly generalized model."
How to find an optimal value of the hyperparameter “ n_tree”?,ML_notes_csv,"Below are some points to remember while selecting the value of K in the K-NN algorithm: There is no particular way to determine the best value for ""K"", so we need to try some values to find the best out of them. The most preferred value for K is 5.A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.Large values for K are good, but it may find some difficulties."
How to find an optimal value of the hyperparameter “mtry”?,ML_notes_csv,"As we have trained our model, our next step is to optimize our model more. Tuning and optimizing helps our model to maximize its performance and generalization ability. This process involves fine-tuning hyperparameters, selecting the best algorithm, and improving features through feature engineering techniques. Hyperparameters are parameters that are set before the training process begins and control the behavior of the machine learning model. These are like learning rate, regularization and parameters of the model should be carefully adjusted. Techniques like grid search cv randomized search and cross-validation are some optimization techniques that are used to systematically explore the hyperparameter space and identify the best combination of hyperparameters for the model. Overall, tuning and optimizing the model involves a combination of careful speculation of parameters, feature engineering, and other techniques to create a highly generalized model."
How do Random Forests select the Important Features?,ML_notes_csv,"Numerous models, such as classifiers are strategically made and combined to solve a specific computational program which is known as ensemble learning. The ensemble methods are also known as committee-based learning or learning multiple classifier systems. It trains various hypotheses to fix the same issue. One of the most suitable examples of ensemble modeling is the random forest trees where several decision trees are used to predict outcomes. It is used to improve the classification, function approximation, prediction, etc. of a model."
Explain the steps of calculating Variable Importance in Random Forest.,ML_notes_csv,"Decision Tree is a supervised learning algorithm which can be used for solving both classification and regression problems.It can solve problems for both categorical and numerical dataDecision Tree regression builds a tree-like structure in which each internal node represents the ""test"" for an attribute, each branch represent the result of the test, and each leaf node represents the final decision or result.A decision tree is constructed starting from the root node/parent node (dataset), which splits into left and right child nodes (subsets of dataset). These child nodes are further divided into their children node, and themselves become the parent node of those nodes. Consider the below image: Above image showing the example of Decision Tee regression, here, the model is trying to predict the choice of a person between Sports cars or Luxury car. Random forest is one of the most powerful supervised learning algorithms which is capable of performing regression as well as classification tasks.The Random Forest regression is an ensemble learning method which combines multiple decision trees and predicts the final output based on the average of each tree output. The combined decision trees are called as base models, and it can be represented more formally as: Random forest usesBagging or Bootstrap Aggregationtechnique of ensemble learning in which aggregated decision tree runs in parallel and do not interact with each other.With the help of Random Forest regression, we can prevent Overfitting in the model by creating random subsets of the dataset."
List down some of the shortcomings of the Random Forest Algorithm.,ML_notes_csv,"The classifier algorithms are designed to indicate whether a new data point belongs to one or another among several predefined classes. Imagine when you are organising emails into spam or inbox, categorising images as cat or dog, or predicting whether a loan applicant is a credible borrower. In the classification models, there is a learning process by the use of labeled examples from each category. In this process, they discover the correlations and relations within the data that help to distinguish class one from the other classes. After learning these patterns, the model is then capable of assigning these class labels to unseen data points. Common Classification Algorithms: Logistic Regression: A very efficient technique for the classification problems of binary nature (two types, for example, spam/not spam). Support Vector Machine (SVM): Good for tasks like classification, especially when the data has a large number of features. Decision Tree: Constructs a decision tree having branches and proceeds to the class predictions through features. Random Forest: The model generates an ""ensemble"" of decision trees that ultimately raise the accuracy and avoid overfitting (meaning that the model performs great on the training data but lousily on unseen data). K-Nearest Neighbors (KNN): Assigns a label of the nearest neighbors for a given data point."
List down the advantages and disadvantages of the Random Forest Algorithm.,ML_notes_csv,"The classifier algorithms are designed to indicate whether a new data point belongs to one or another among several predefined classes. Imagine when you are organising emails into spam or inbox, categorising images as cat or dog, or predicting whether a loan applicant is a credible borrower. In the classification models, there is a learning process by the use of labeled examples from each category. In this process, they discover the correlations and relations within the data that help to distinguish class one from the other classes. After learning these patterns, the model is then capable of assigning these class labels to unseen data points. Common Classification Algorithms: Logistic Regression: A very efficient technique for the classification problems of binary nature (two types, for example, spam/not spam). Support Vector Machine (SVM): Good for tasks like classification, especially when the data has a large number of features. Decision Tree: Constructs a decision tree having branches and proceeds to the class predictions through features. Random Forest: The model generates an ""ensemble"" of decision trees that ultimately raise the accuracy and avoid overfitting (meaning that the model performs great on the training data but lousily on unseen data). K-Nearest Neighbors (KNN): Assigns a label of the nearest neighbors for a given data point."
What is the curse of dimensionality?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
Define Principal Component Analysis (PCA)?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
Can Principal Component Analysis be used in Feature Selection?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
How to select the first principal component axis?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
What does a Principal Component Analysis’s major component represent?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
What are the disadvantages of dimension reduction?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
Why do we standardize before using Principal Component Analysis?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
What happens when the eigenvalues are nearly equal?,ML_notes_csv,"Ridge regression is one of the most robust versions of linear regression in which a small amount of bias is introduced so that we can get better long term predictions.The amount of bias added to the model is known asRidge Regression penalty. We can compute this penalty term by multiplying with the lambda to the squared weight of each individual features.The equation for ridge regression will be: A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called asL2 regularization.It helps to solve the problems if we have more parameters than samples."
What happens if the PCA components are not rotated?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
Can we implement Principal Component Analysis for Regression?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
Can PCA be used on Large Datasets?,ML_notes_csv,"Sometimes it is difficult to both visualize and analyze the data when you have a large feature space (dimensions). The purpose of dimensionality reduction methods is to decrease the dimensions needed to maintain the key features. Dimensions of greatest importance are identified by principal component analysis (PCA), which is the reason why data is concentrated in fewer dimensions with the highest variations. This speeds up model training as well as offers a chance for more efficient visualization. LDA (Linear Discriminant Analysis) also resembles PCA but it is made for classification tasks where it concentrates on dimensions that can differentiate the present classes in the dataset."
How is PCA used to detect anomalies?,ML_notes_csv,"Unsupervised learning can also be applied to find those data points which greatly differ than the majorities. The statistics model may identify these outliers, or anomalies as signaling of errors, fraud or even something unusual. Local Outlier Factor (LOF) makes a comparison of a given data point's local density with those surrounding it. It then flags out the data points with significantly lower densities as outliers or potential anomalies. Isolation Forest is the one which uses different approach, which is to recursively isolate data points according to their features. Anomalies usually are simple to contemplate as they often necessitate fewer steps than an average normal point."
